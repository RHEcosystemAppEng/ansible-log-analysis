# Backend Configuration
backend:
  name: alm-backend
  image:
    repository: ansible-log-monitor-backend
    tag: "latest"
    pullPolicy: IfNotPresent
  
  replicas: 2
  
  service:
    type: ClusterIP
    port: 8000
    targetPort: 8000
  
  ingress:
    enabled: true
    className: "nginx"
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /
    hosts:
      - host: alm-backend.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
  
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 500m
      memory: 512Mi
  
  env:
    - name: DATABASE_URL
      valueFrom:
        secretKeyRef:
          name: pgvector
          key: uri
    - name: OPENAI_API_TOKEN
      valueFrom:
        secretKeyRef:
          name: alm-backend-secrets
          key: openai-api-token
    - name: OPENAI_API_ENDPOINT
      value: "https://api.openai.com/v1"
    - name: OPENAI_MODEL
      value: "gpt-4o-mini"
    - name: LANGSMITH_TRACING
      value: "false"
    - name: LANGSMITH_API_KEY
      valueFrom:
        secretKeyRef:
          name: alm-backend-secrets
          key: langsmith-api-key
    - name: LANGSMITH_PROJECT
      value: "ansible-log-monitor"
    - name: BACKEND_URL
      value: "http://alm-backend:8000"
  
  envFrom:
    - secretRef:
        name: alm-backend-secrets
  
  livenessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 30
    periodSeconds: 10
  
  readinessProbe:
    httpGet:
      path: /health
      port: 8000
    initialDelaySeconds: 5
    periodSeconds: 5

# UI Configuration
ui:
  name: alm-ui
  image:
    repository: ansible-log-monitor-ui
    tag: "latest"
    pullPolicy: IfNotPresent
  
  replicas: 2
  
  service:
    type: ClusterIP
    port: 7860
    targetPort: 7860
  
  ingress:
    enabled: true
    className: "nginx"
    annotations:
      nginx.ingress.kubernetes.io/rewrite-target: /
    hosts:
      - host: alm-ui.local
        paths:
          - path: /
            pathType: Prefix
    tls: []
  
  # Horizontal Pod Autoscaler
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80
    # Optional: Custom scaling behavior
    behavior:
      scaleUp:
        stabilizationWindowSeconds: 30
        policies:
        - type: Percent
          value: 100
          periodSeconds: 60
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
        - type: Percent
          value: 50
          periodSeconds: 60
  
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 200m
      memory: 256Mi
  
  # Environment variables (BACKEND_URL is automatically set)
  env:
    - name: BACKEND_URL
      value: "http://alm-backend:8000"
  
  # Environment variables from ConfigMaps/Secrets
  envFrom: []
  
  # Health probes
  livenessProbe:
    httpGet:
      path: /
      port: 7860
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 3
  
  readinessProbe:
    httpGet:
      path: /
      port: 7860
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 5
    failureThreshold: 3

# PostgreSQL Configuration
postgres:
  enabled: true
  name: postgres
  image:
    repository: postgres
    tag: "15"
  
  service:
    type: ClusterIP
    port: 5432
  
  env:
    - name: POSTGRES_USER
      value: "user"
    - name: POSTGRES_PASSWORD
      value: "password"
    - name: POSTGRES_DB
      value: "logsdb"
  
  persistence:
    enabled: true
    size: 10Gi
    accessMode: ReadWriteOnce

# Secrets Configuration
secrets:
  # OpenAI API Token - set this to your actual OpenAI API key
  openaiApiToken: "your-openai-api-token-here"
  
  # LangSmith API Key - set this to your actual LangSmith API key  
  langsmithApiKey: "your-langsmith-api-key-here"

# Global settings
global:
  storageClass: ""
  nodeSelector: {}
  tolerations: []
  affinity: {}

